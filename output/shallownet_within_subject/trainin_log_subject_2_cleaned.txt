  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur
-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------
      1            0.2500        5.9110       0.2500            0.2500       30.7352  0.0200  8.4718
      2            0.3611        4.5099       0.2257            0.2257        9.6617  0.0200  10.7008
      3            0.4896        3.5350       0.2708            0.2708        4.8101  0.0200  10.1730
      4            0.3924        3.1140       0.2535            0.2535       13.3293  0.0200  9.6494
      5            0.5208        3.2288       0.2396            0.2396        8.0578  0.0199  8.6095
      6            0.5694        3.4629       0.2743            0.2743        4.7016  0.0199  9.1060
      7            0.6458        2.7389       0.2292            0.2292        5.8031  0.0198  8.6646
      8            0.6771        2.7611       0.2222            0.2222        8.4622  0.0198  8.6214
      9            0.6250        2.8244       0.2847            0.2847       11.1364  0.0197  8.4121
     10            0.7292        2.8362       0.2708            0.2708        7.3282  0.0196  9.2220
     11            0.6632        2.2019       0.2431            0.2431        7.9542  0.0195  8.2543
     12            0.7500        2.4519       0.2292            0.2292        6.2805  0.0194  8.6632
     13            0.7326        2.3207       0.2188            0.2188       10.2602  0.0193  7.8436
     14            0.7812        1.9484       0.2257            0.2257        6.0703  0.0192  8.7615
     15            0.7951        2.1243       0.2465            0.2465        6.4889  0.0190  8.5325
     16            0.8160        1.4919       0.2535            0.2535        6.0870  0.0189  8.2503
     17            0.8125        1.6766       0.2292            0.2292        5.4094  0.0187  8.9008
     18            0.9236        1.3915       0.2604            0.2604        7.9700  0.0186  9.7012
     19            0.8715        1.3771       0.2708            0.2708        6.5800  0.0184  8.7174
     20            0.9271        1.4123       0.2500            0.2500        5.6456  0.0182  8.6353
     21            0.9375        1.4174       0.2604            0.2604        6.1087  0.0181  8.5197
     22            0.8993        1.2645       0.2639            0.2639        5.4969  0.0179  8.9558
     23            0.9410        1.2105       0.2292            0.2292        5.8014  0.0177  8.8614
Stopping since valid_loss has not improved in the last 18 epochs.
Elapsed time: 270.48646330833435 seconds
Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']
Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']
Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']
Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']
Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']
Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']
Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']
Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']
Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']
Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']
Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']
Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']
============================================================================================================================================
Layer (type (var_name):depth-idx)        Input Shape               Output Shape              Param #                   Kernel Shape
============================================================================================================================================
ShallowFBCSPNet (ShallowFBCSPNet)        [1, 22, 1125]             [1, 4]                    --                        --
├─Ensure4d (ensuredims): 1-1             [1, 22, 1125]             [1, 22, 1125, 1]          --                        --
├─Rearrange (dimshuffle): 1-2            [1, 22, 1125, 1]          [1, 1, 1125, 22]          --                        --
├─CombinedConv (conv_time_spat): 1-3     [1, 1, 1125, 22]          [1, 40, 1101, 1]          36,240                    --
├─BatchNorm2d (bnorm): 1-4               [1, 40, 1101, 1]          [1, 40, 1101, 1]          80                        --
├─Expression (conv_nonlin_exp): 1-5      [1, 40, 1101, 1]          [1, 40, 1101, 1]          --                        --
├─AvgPool2d (pool): 1-6                  [1, 40, 1101, 1]          [1, 40, 69, 1]            --                        [75, 1]
├─Expression (pool_nonlin_exp): 1-7      [1, 40, 69, 1]            [1, 40, 69, 1]            --                        --
├─Dropout (drop): 1-8                    [1, 40, 69, 1]            [1, 40, 69, 1]            --                        --
├─Sequential (final_layer): 1-9          [1, 40, 69, 1]            [1, 4]                    --                        --
│    └─Conv2d (conv_classifier): 2-1     [1, 40, 69, 1]            [1, 4, 1, 1]              11,044                    [69, 1]
│    └─LogSoftmax (logsoftmax): 2-2      [1, 4, 1, 1]              [1, 4, 1, 1]              --                        --
│    └─Expression (squeeze): 2-3         [1, 4, 1, 1]              [1, 4]                    --                        --
============================================================================================================================================
Total params: 47,364
Trainable params: 47,364
Non-trainable params: 0
Total mult-adds (Units.MEGABYTES): 0.01
============================================================================================================================================
Input size (MB): 0.10
Forward/backward pass size (MB): 0.35
Params size (MB): 0.04
Estimated Total Size (MB): 0.50
============================================================================================================================================
