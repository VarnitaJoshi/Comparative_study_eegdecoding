  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur
-------  ----------------  ------------  -----------  ----------------  ------------  ------  ------
      1            0.2500        4.2361       0.2500            0.2500       25.4955  0.0200  9.2109
      2            0.3576        4.6987       0.3056            0.3056        6.1297  0.0200  8.6069
      3            0.5104        2.9688       0.3160            0.3160        4.3850  0.0200  9.3552
      4            0.4201        2.9255       0.2431            0.2431        5.3791  0.0200  8.2967
      5            0.6354        2.6541       0.3125            0.3125        4.6297  0.0199  8.7299
      6            0.7257        2.1917       0.3750            0.3750        3.6525  0.0199  8.9769
      7            0.7917        1.9813       0.3715            0.3715        4.2357  0.0198  8.5581
      8            0.7917        1.6739       0.3333            0.3333        4.4302  0.0198  9.0660
      9            0.8785        1.5669       0.3681            0.3681        3.7482  0.0197  6.4781
     10            0.8819        1.5543       0.3576            0.3576        4.2185  0.0196  9.4540
     11            0.9028        1.1624       0.3611            0.3611        4.4810  0.0195  9.1122
     12            0.8993        1.6669       0.3507            0.3507        4.5666  0.0194  10.1651
     13            0.9271        1.2413       0.4167            0.4167        4.2557  0.0193  9.1365
     14            0.9375        0.9293       0.3854            0.3854        4.8418  0.0192  9.5112
     15            0.9653        0.7108       0.3542            0.3542        4.8576  0.0190  9.5979
     16            0.9583        0.9086       0.4028            0.4028        4.5340  0.0189  9.2808
     17            0.9514        1.0113       0.3924            0.3924        4.8281  0.0187  9.9210
     18            0.9792        0.9637       0.3993            0.3993        4.9171  0.0186  10.3526
     19            0.9688        1.0378       0.4306            0.4306        4.7064  0.0184  9.0684
     20            0.9583        0.6051       0.4062            0.4062        4.9334  0.0182  10.6916
     21            0.9826        0.8843       0.4167            0.4167        4.6773  0.0181  10.5532
     22            0.9722        0.6652       0.4167            0.4167        4.9026  0.0179  10.3393
     23            0.9861        0.7146       0.4306            0.4306        4.8917  0.0177  9.2845
Stopping since valid_loss has not improved in the last 18 epochs.
Elapsed time: 280.0989017486572 seconds
Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']
Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']
Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']
Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']
Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']
Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']
Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']
Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']
Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']
Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']
Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']
Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']
============================================================================================================================================
Layer (type (var_name):depth-idx)        Input Shape               Output Shape              Param #                   Kernel Shape
============================================================================================================================================
ShallowFBCSPNet (ShallowFBCSPNet)        [1, 22, 1125]             [1, 4]                    --                        --
├─Ensure4d (ensuredims): 1-1             [1, 22, 1125]             [1, 22, 1125, 1]          --                        --
├─Rearrange (dimshuffle): 1-2            [1, 22, 1125, 1]          [1, 1, 1125, 22]          --                        --
├─CombinedConv (conv_time_spat): 1-3     [1, 1, 1125, 22]          [1, 40, 1101, 1]          36,240                    --
├─BatchNorm2d (bnorm): 1-4               [1, 40, 1101, 1]          [1, 40, 1101, 1]          80                        --
├─Expression (conv_nonlin_exp): 1-5      [1, 40, 1101, 1]          [1, 40, 1101, 1]          --                        --
├─AvgPool2d (pool): 1-6                  [1, 40, 1101, 1]          [1, 40, 69, 1]            --                        [75, 1]
├─Expression (pool_nonlin_exp): 1-7      [1, 40, 69, 1]            [1, 40, 69, 1]            --                        --
├─Dropout (drop): 1-8                    [1, 40, 69, 1]            [1, 40, 69, 1]            --                        --
├─Sequential (final_layer): 1-9          [1, 40, 69, 1]            [1, 4]                    --                        --
│    └─Conv2d (conv_classifier): 2-1     [1, 40, 69, 1]            [1, 4, 1, 1]              11,044                    [69, 1]
│    └─LogSoftmax (logsoftmax): 2-2      [1, 4, 1, 1]              [1, 4, 1, 1]              --                        --
│    └─Expression (squeeze): 2-3         [1, 4, 1, 1]              [1, 4]                    --                        --
============================================================================================================================================
Total params: 47,364
Trainable params: 47,364
Non-trainable params: 0
Total mult-adds (Units.MEGABYTES): 0.01
============================================================================================================================================
Input size (MB): 0.10
Forward/backward pass size (MB): 0.35
Params size (MB): 0.04
Estimated Total Size (MB): 0.50
============================================================================================================================================
